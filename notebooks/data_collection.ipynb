{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e98aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulation data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#visualiation data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import matplotlib\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "\n",
    "#from dataprep.eda import *\n",
    "#from dataprep.datasets import load_dataset\n",
    "#from dataprep.eda import create_report\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import r2_score, classification_report\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import math\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "\n",
    "\n",
    "import datetime as dt\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c927000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d7d490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(client_id ='StHnWUG6a7qOISVKpPbUEw',\n",
    "client_secret ='R8ezceqUsbCC8Vx8zPRS34jjAFxphQ',\n",
    "user_agent ='sentiment analysis')\n",
    "\n",
    "print(reddit.read_only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e289dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30000\n",
    "num_batches = 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00cd0b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('photography')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b6ef75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"../data/text_based_posts.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f500f91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching batch 1/10\n",
      "Fetching batch 2/10\n",
      "Fetching batch 3/10\n",
      "Fetching batch 4/10\n",
      "Fetching batch 5/10\n",
      "Fetching batch 6/10\n",
      "Fetching batch 7/10\n",
      "Fetching batch 8/10\n",
      "Fetching batch 9/10\n",
      "Fetching batch 10/10\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "     for batch_num in range(num_batches):\n",
    "            print(f\"Fetching batch {batch_num + 1}/{num_batches}\")\n",
    "\n",
    "            retry_attempts = 3\n",
    "            for attempt in range(retry_attempts):\n",
    "                try:\n",
    "                    posts = subreddit.new(limit=batch_size)\n",
    "\n",
    "                    for post in posts:\n",
    "                        if post.selftext:\n",
    "                            output_file.write(f\"Text Content: {post.selftext}\\n\")\n",
    "\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    print(f\"Retrying ({attempt + 1}/{retry_attempts})...\")\n",
    "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
    "\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fefbf18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
